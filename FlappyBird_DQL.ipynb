{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let DQN play Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flappy bird was a popular game for its high difficulty with an easy-understanding control. What the player needs to do is tapping the screen to make the bird fly higher or doing nothing to drop the bird, in order to let the bird fly over pipes. Though the control is easy, getting good scores is a hard problem. Then training an AI agent who is able to perfectly play this game would be a really interesting project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/FlappyBird.jpg\"  width = 200, height = 100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group though it is a great model for us to learn how DQN works, since in this specific game, an agent only has two actions in each state and the description of the state can be simplified as several parameters. Thatâ€™s might be the reason why so many DQN tutorials use Flappy Bird as the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Game Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game enviroment, found on github, is a python based Flappy Bird version. The funtion below is the main funtion in this game which is used to update the game state and display the game screen. We have modified the parameters and the states to accelerate the training process. The return value \"info\" is game state in Q Learning.\n",
    "\n",
    "During the game, in most of time, there will be three pairs of pipes displayed on the screen which is different from the intial conditon. We handle this exception by judging the size of Pipes array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frame_step(self, input_actions):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 1.0\n",
    "        terminal = False\n",
    "\n",
    "        # input_actions[0] == 1: do nothing\n",
    "        # input_actions[1] == 1: flap the bird\n",
    "        if input_actions[1] == 1:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                reward = 1.0\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        \n",
    "        if isCrash:\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1000\n",
    "\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        \n",
    "        if len(self.lowerPipes) == 2:\n",
    "            info = np.array([self.playery,self.playerVelY , self.lowerPipes[0]['x'],self.lowerPipes[0]['y'],self.upperPipes[0]['y']])\n",
    "        else:\n",
    "            info = np.array([self.playery, self.playerVelY, self.lowerPipes[1]['x'], self.lowerPipes[1]['y'],self.upperPipes[1]['y']])\n",
    "            \n",
    "        return info, reward, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game enviroment provides a lot of useful interface for DQN training. For example, \"FPS\" allows user to boost the game speed which will greatly accelerate the training process, and user can block \"display_update\" to prevent the waste of GPU rescource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design of Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The structuce of DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we use for DQN is Deep Q-Learning with experience replay. It requires two Neural Network with the same structure, and one of them are used for trainging the Q-table model and the other one which should be updated with a delay is used to estimate the optimal actions in one specific state and provide the traing set.\n",
    "\n",
    "We have tried several Neural Network models, and the model delivering a good result is constructed as the code show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_net(self):\n",
    "# ------------------ build evaluate_net ------------------   \n",
    "self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input    \n",
    "self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss    \n",
    "    with tf.variable_scope('eval_net'):      \n",
    "        # c_names(collections_names) are the collections to store variables\n",
    "        c_names, n_l1, n_h0, n_h1, n_h2, w_initializer, b_initializer = \\\n",
    "        ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 500, 500, 500, 500, \\\n",
    "        tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "        \n",
    "        # first layer. collections is used later when assign to target net\n",
    "        with tf.variable_scope('l1'):\n",
    "            w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "            l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "            \n",
    "        # hidden layer 0\n",
    "        with tf.variable_scope('l3'):\n",
    "            w3 = tf.get_variable('w3', [n_l1, n_h0], initializer=w_initializer, collections=c_names)\n",
    "            b3 = tf.get_variable('b3', [1, n_h0], initializer=b_initializer, collections=c_names)\n",
    "            l3 = tf.nn.relu(tf.matmul(l1, w3) + b3)\n",
    "            \n",
    "        # hidden layer 1\n",
    "        with tf.variable_scope('l4'):\n",
    "            w4 = tf.get_variable('w4', [n_h0, n_h1], initializer=w_initializer, collections=c_names)\n",
    "            b4 = tf.get_variable('b3', [1, n_h1], initializer=b_initializer, collections=c_names)\n",
    "            l4 = tf.nn.relu(tf.matmul(l3, w4) + b4)\n",
    "        \n",
    "        # hidden layer 2\n",
    "        with tf.variable_scope('l5'):\n",
    "            w5 = tf.get_variable('w5', [n_h1, n_h2], initializer=w_initializer, collections=c_names)\n",
    "            b5 = tf.get_variable('b5', [1, n_h2], initializer=b_initializer, collections=c_names)\n",
    "            l5 = tf.nn.relu(tf.matmul(l4, w5) + b5)\n",
    "            \n",
    "        # second layer. collections is used later when assign to target net\n",
    "        with tf.variable_scope('l2'):\n",
    "            w2 = tf.get_variable('w2', [n_h2, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "            b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "            self.q_eval = tf.matmul(l5, w2) + b2\n",
    "            \n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        \n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a Neural Network with 4 hidden layers with one input layer, and, in each hidden layer, there are 500 Relu units. The Neural Network strcuture graph generted by Tensorboard is show below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/NN_structure.png\"  width = 900, height = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Tuning of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Neural Network's parameters is the hardest part of a Deep Learning project. \n",
    "\n",
    "First of all, the parameters of each unit in neural network is initialized by Tensorflow functions. We have already done this part while we are buling the network.\n",
    "\n",
    "In next stept, we start to mannualy tune other parameters including learning rate, the punishment of a crash, memory size, epsilon and update size. These parameter are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # bird game\n",
    "    env = game.GameState()  # define the game environment -> jump to game\n",
    "    actions = 2\n",
    "    features = 5\n",
    "    RL = DeepQNetwork(actions, features, \n",
    "                      learning_rate= 10**-2,\n",
    "                      reward_decay=1.0, e_greedy=0.6,\n",
    "                      replace_target_iter= 200,\n",
    "                      memory_size=50,\n",
    "                      output_graph=True)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    run_bird()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tensorfflow to save each layer's parameters every 5000 steps, and the initial learning rate is 0.01. During the training, we monitor the effect of model all the time. When the AI  bird can pass several pipes, we then pause the trainging process and use a smaller learning rate to continue training the model. The memory size can be tuned smaller if the graph of loss fiunction did not proper decrease. Though we only concern the game score, the loss function also matters in optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trainging Model part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"video/Normal_SameHigh.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"video/Normal_SameHigh.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    #gapY = gapYs[index]\n",
    "    gapY = gapYs[0]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT*1.2},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE*1.2},  # lower pipe\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first set the gapY to a constant value. As you can see in the video, all the pipes are at the same height which accelerate the training process. We want to fasten the training process so that we can easily test whether our network comes into effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training Model part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"video/Normal_Random.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"video/Normal_Random.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "index = random.randint(0, len(gapYs)-1)  \n",
    "gapY = gapYs[index]  \n",
    "```\n",
    "Here we set the gapY to a random value so in the video you can see the height of the pipes are different. In the previous model, the position of the pipes doesn't matter but here they are important states.   \n",
    "```\n",
    "{'x': pipeX, 'y': gapY - PIPE_HEIGHT*1.2},  \n",
    "{'x': pipeX, 'y': gapY + PIPEGAPSIZE*1.2},  \n",
    "```\n",
    "Considering the time limit, the window size we set is 1.2 times of the initial version. This also accelerate the learning process and we have time to apply different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original game model: https://github.com/floodsung/DRL-FlappyBird/blob/master/game/wrapped_flappy_bird.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original DQN model: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Origianl Q learning idea from Jeremy's lecture notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
